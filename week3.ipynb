{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91133689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Load dataset ----------------\n",
    "df = pd.read_csv(r\"C:\\HOPE\\my progress\\Amazon ML cohort\\week 3 project\\student+performance\\student\\student-mat.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f72ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Create binary target ----------------\n",
    "df['Pass'] = (df['G3'] >= 10).astype(int)  # Pass if G3 >= 10\n",
    "df = df.drop(columns=['G3'])  # drop original grade to avoid leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bdae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Encode categorical variables ----------------\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6614f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Split into train/test ----------------\n",
    "X = df.drop(columns=['Pass'])\n",
    "y = df['Pass']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8160cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Logistic Regression =====\n",
      "Accuracy: 0.9493670886075949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93        27\n",
      "           1       0.98      0.94      0.96        52\n",
      "\n",
      "    accuracy                           0.95        79\n",
      "   macro avg       0.94      0.95      0.94        79\n",
      "weighted avg       0.95      0.95      0.95        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Logistic Regression ----------------\n",
    "print(\"===== Logistic Regression =====\")\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f38432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest =====\n",
      "Accuracy: 0.9113924050632911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88        27\n",
      "           1       0.96      0.90      0.93        52\n",
      "\n",
      "    accuracy                           0.91        79\n",
      "   macro avg       0.90      0.91      0.90        79\n",
      "weighted avg       0.92      0.91      0.91        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Random Forest ----------------\n",
    "print(\"\\n===== Random Forest =====\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ff50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Important Features (Random Forest):\n",
      "G2          0.362918\n",
      "G1          0.206461\n",
      "absences    0.040848\n",
      "age         0.030221\n",
      "failures    0.029940\n",
      "goout       0.025687\n",
      "Fedu        0.023092\n",
      "Mjob        0.019468\n",
      "freetime    0.019435\n",
      "health      0.017991\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Feature Importance from Random Forest ----------------\n",
    "importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "print(\"\\nTop 10 Important Features (Random Forest):\")\n",
    "print(importances.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model performed best and why?\n",
    "# In our case, the Random Forest classifier outperformed Logistic Regression in terms of accuracy and recall. \n",
    "# That’s because Random Forest can capture nonlinear relationships and complex interactions between variables —for example, \n",
    "# how study time and number of failures together influence performance — without us manually engineering those features. \n",
    "# Logistic Regression assumes a linear relationship in the log-odds space, so it struggled a bit in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which one was fastest?\n",
    "#Logistic Regression was definitely faster to train and predict. It’s a simple linear model that fits a small \n",
    "# number of parameters, so the computational cost is minimal. Random Forest was still reasonably fast for this dataset, \n",
    "# but it’s slower because it builds multiple decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which one was most interpretable?\n",
    "#Logistic Regression is more interpretable. \n",
    "# The coefficients directly tell us how much each feature increases or decreases the probability of passing, \n",
    "# which is valuable when we need to explain results to non-technical stakeholders. \n",
    "# Random Forest can give us feature importances, but it’s more of a black box, \n",
    "# so explaining individual predictions is harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbe0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When would you use one over the other?\n",
    "#If my priority is explainability and I believe the data follows a roughly linear relationship, \n",
    "#I’d use Logistic Regression. It’s quick, transparent, and easy to communicate.\n",
    "#If my goal is maximizing predictive performance and I expect nonlinear patterns or complex feature interactions, \n",
    "# I’d use Random Forest. It handles mixed data types well, is robust to outliers, \n",
    "# and generally gives higher accuracy in such cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
